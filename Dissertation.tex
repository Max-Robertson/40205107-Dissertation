%You can delete all the comments after you have finished your document
%this sets up the defaults for the documents, 12pt font and A4 size. The article type sets this up as such as opposed to letter or memo.

%for the finer points LaTeX see https://en.wikibooks.org/wiki/LaTeX or http://tex.stackexchange.com/

\documentclass[12pt,a4paper]{article}
\usepackage{titlesec} %these are how we import packages, one helps set up footers and title layout
\usepackage{fancyhdr}

% !TEX TS-program = pdflatex
% !TEX encoding = UTF-8 Unicode
\usepackage[utf8]{inputenc} % set input encoding (not needed with XeLaTeX)

\usepackage{graphicx} % support the \includegraphics command and options

% \usepackage[parfill]{parskip} % Activate to begin paragraphs with an empty line rather than an indent

%%% PACKAGES
\usepackage{booktabs} % for much better looking tables
\usepackage{array} % for better arrays (eg matrices) in maths
\usepackage{paralist} % very flexible & customisable lists (eg. enumerate/itemize, etc.)
\usepackage{verbatim} % adds environment for commenting out blocks of text & for better verbatim
\usepackage{subfig} % make it possible to include more than one captioned figure/table in a single float
\usepackage[toc,page]{appendix}
% These packages are all incorporated in the memoir class to one degree or another...

%header and footer settings
\pagestyle{fancyplain}
\fancyhf{}
\renewcommand{\headrulewidth}{0.5pt}
\renewcommand{\footrulewidth}{0.5pt}
\setlength{\headheight}{15pt} 
\setlength{\parskip}{1em}
\fancyhead[L]{Max Robertson - 40205107}
\fancyhead[R]{ SOC10101 Honours Project}
\fancyfoot[L]{}
\fancyfoot[C]{\thepage}

%set better section layout
\makeatletter
\renewcommand\subsection{\@startsection {subsection}{1}{2mm} % name, level, indent
                               {3pt plus 2pt minus 1pt} % before skip
                               {3pt plus 0pt} % after skip
                               {\normalfont\bfseries}}
\makeatother
\makeatletter
\renewcommand\section{\@startsection {section}{1}{0mm} % name, level, indent
                               {4pt plus 2pt minus 1pt} % before skip
                               {4pt plus 0pt} % after skip
                               {\bfseries}}
\makeatother


%this starts the document
\begin{document}

%you can import other documents into your main one, these layout the Title and Declarations on its own page.
%you might need to change these to \ if your on Microsoft Windows.
\input{./Dissertation-Title.tex}
\input{./Dissertation-Dec.tex}
\pagebreak
\input{./Dissertation-DP.tex}
\pagebreak

%LaTeX let you define the abstract separately so it wont get sucked into the main document.
\begin{abstract}
% fill the abstract in here
\end{abstract}
\pagebreak

\tableofcontents % is generated for you
\newpage

\listoftables
%generated in same way as figures
\newpage

\listoffigures
%you may have captions such as equations, listings etc they should all appear as required
%these are done for you as long as you use \begin{figure}[placement settings] .. bla bla ... \end{figure}
\newpage

\section*{Acknowledgements}
Insert acknowledgements here
\subsection*{}
	I would like to thank my cat, dog and family.
\newpage


\section{Introduction} 
\subsection{Background}  
In recent years a shortage of IT skills has been seen, however a more severe shortage has been prevalent with Cyber security skills. With statistics like 45 percent of organizations claiming to be lacking in cyber security skills and (found by research done by the Information Systems Security Association) 70 percent of IT workers believe the lack of cyber security professionals is negatively impacting their existing work \cite{smith2018intelligent}. This has been a widely discussed issue in recent years and there has been a number of proposals for how to fix this 'cyber security skills gap' from developing the companies HR department so that an individual interested in gaining the relevant skills is given the appropriate training by the company to advancing security technology and crime deterrence so that less personnel are needed to close the gap \cite{cobb2016mind}. I however believe that an emphasis should be put on cyber security at an earlier age and high school students should be introduced to cyber security elements. If students are introduced to these elements at an there is more of a chance of capturing an interest in the subject and for  students to pursue a career in cyber security. Further more I believe if research is done in how to cater this material to the students and how to peak their interests the effect could be greater. This could be done through the use of gamification which has been proposed as a solution for engaging people in individually and socially sustainable behaviours, such as education \cite{su2015mobile}. CTF (Capture The Flag) competitions are held in cyber security communities which are essentially a gamfied environment where teams compete by completing different cyber security related challenges to ascertain 'flags' which will score them points. This project proposed to take the structure of a CTF competition and gamify it further for use in the education of cyber security.  
\subsection{Aims} 
As the area of 'the gamification of cyber security education' has been establishes as the area of research I shall establish some aims and objectives that I hope to achieve throughout the process of this project:  


My first aim is to gain an understanding of different learning techniques that can I could apply when implementing my project as well as have an in depth understanding of the concepts that I propose to utilize during the implementation. I will achieve this by doing extensive research and produce a literature review, breaking down different learning techniques. I will also further look into CTFs and the associated available technologies to give a better understanding of how they work and how I could utilize aspects of them in implementation as well as a look into gamification, how to make use of the positives and avoid any pitfalls.  

The second aim of this Honours project will be take the research done in the literature review and produce a tool that will help educate users in cyber security as well as engaging them in the subject as a whole all while making it easy to understand. To achieve this aim I will complete the following objectives. 
\begin{itemize}\itemsep0pt
	\item Develop the application with reference to research.
	\item Develop learning materials to be used in conjunction with the application.
	\item Develop a web page to be used in conjunction with the application.
\end{itemize}


\subsection{Dissertation Structure}




\section{Literature Review}
insert my lit here  
\subsection{summary}
% summarise my literature review here 

 
% possibly talk about and explain randomness, random number generator ? how am i generating randomness ? 

\section{Design}
\subsection{Introduction}  
The literature has backed up the need for such an application, now I will look at how the areas covered can be implemented in the design of the application and go on to discuss how the application will be designed in its structure taking account what has been discovered in the literature. I will then  discuss the design of some lessons that application could contain and then go into detail about the technical design of the application. I will take into account methods of research and consider what the best approach to take is to meet the aims of the project. 

\subsection{Requirements analysis} 
After doing the appropriate research on the topic, and designing the concept for the application I then looked into how best to carry out the research to meet my aims and objectives. Williams defines qualitative research as allowing the user to explore and better understand the phenomenon whereas quantitative research provides and objective measure of reality \cite{williams2007research}. Greenbaum has further discussed these research methods \cite{greenbaum1999moderating} and cited Michiello's table as useful for deciding a primary research method (Figure \ref{Research}). 

\begin{figure}[h]
    \centering
    \includegraphics[width=0.75\textwidth]{Figs/research.PNG} 
    \caption{Quantitative vs Qualitative} 
    \label{Research}
\end{figure}  

As the table shows the qualitative method would involve me studying the human interaction side of things, e.g making the application and through the form of questionnaires, getting data on how the application was useful. A quantitative method involves measuring things, numerical comparisons, discovering facts etc This would involve prototyping aspects of the application and measuring their use. Both methods would be valid for meeting the aims and objectives of the project if designed right. However the qualitative method in this case would involve testing the application on the target audience (high school students 
) and would therefore throw ethics into consideration. Another issue with the qualitative method is if I was planning to do a questionnaire to gather data on the effectiveness of the application, that questionnaire should really have  started to be designed earlier in the project life. The quantitative method however, could be useful in after theoretically designing the app prototyping components of it and gauging how useful it would be in achieving the desired effect by testing and measuring the technical performance of the prototyped components. This would help give insight into the proposed use of the technology and if it should be considered for this use. For this reason I have decided to choose the quantitative method of research and do a technical evaluation on prototyped elements of the proposed application.


\subsection{Application Design}  
To design the structure of the the application, and how the rooms/levels would be spaced out I used the literature to design an application that would effectively teach cyber security skills and knowledge. 

Firstly I would separate each “lesson” into a room in the game. A series of these rooms or “lessons” would make up a floor. These rooms would cover and teach different topics of cyber security. Instead of having floors cover different topics, which is how I had originally envisioned the application, the literature suggests that blooms taxonomy Is an effective teaching model to apply to cyber security education so each floor could represent a different level on the blooms taxonomy level of understanding. So as the user would progress up the floors the questions would get more complex and require deeper thought. This structure is illustrated in figure *.  This would allow the application to impart skills in an organised manor so that the user would get the most out of it and by the end of the application would be at the end of the blooms taxonomy understanding scale (change so makes more sense)

%insert figure here 

Another aspect to consider in regards to the structure is that putting “lessons” / rooms back to back requiring the user to complete the lesson in order to progress to the next would be a bad idea. This has been shown to stunt the learning of younger audiences and discourage them from continuing if they can’t get the answer. The literature backed this up implying that freedom of choice is an important factor in gamification and that it encourages the user to explore more and be more engaged with the learning process. To get around this the application feature something called “reward tokens” which the user would get every time they complete a room/lesson. Say there are 5 rooms on a floor, the user would need to obtain 3 reward tokens to proceed to the next floor. Therefore giving the user the freedom to choose which 3 lessons to complete. The user would also need to be stopped from just all 5 lessons on the first floor then only 1 in the second to progress, a solution to this would be to double the reward tokens the user gets for completing a lesson as the floors ascend and adjust the reward tokens needed to go up a floor according, this is illustrated in figure *. 

%insert figure here 

The user would also need to not be discouraged if they for example could only complete two the lessons and is stuck on completing a third on a floor. Having looked over extensive literature on feedback it suggests that just telling the user that they are wrong isn’t very effective but that giving the user some form of hint is beneficial for the user allowing them to not give up if they don’t know the answer but be pointed in the right direction. For this reason a hint system is included in the theoretical design of the application.  If the user unsuccessfully attempts a lesson 3 times then a hint room could unlock. 

The rooms could be designed to incorporate all of these features. In a typical room there would be an opening in every direction. One to the left taking the user to the previous room, one to the right taking the user to the next room, one above which will unlock when the user gets the question right and takes them to the reward room where they can collect the reward tokens and finally one below which would unlock on the third incorrect attempt taking the user to the hint room. This has been visualised in figure *insert figure here*  

The user would play as “anonybot” a robot working for moral Corp who when doing his job discovered they were unlawfully harvesting and processing the personal data of customers. When he confronted his superior about it he was locked down on the bottom floor of the headquarters. The user must play as anonybot as he ascends the floors so he can escape the HQ and out the company for what they are doing.

The inclusion of a character, story and an end goal to reach it all factors which have been cited as useful in engaging a younger audience with learning material, investing them and making them want to reach the end goal by the literature. As this application is aimed at the younger high school audience, with the aim of engaging them in cyber security this is also included in the theoretical design. 

So in summary the application would essentially be a gamified cyber security application, structured similarly to capture the flag (or CTF) events with the freedom of choice operating as the jeopardy style of CTF shown to be effective in engagement, the lessons serving as the challenges and the reward token serving as a flag. The hint system in place is also represented in typical jeopardy style CTFs. The difference is the character and story orientated take and the elevation of floors representing a respective level of Blooms Taxonomy. 
%discuss how the game would be structured levels, floors etc
\subsection{Lesson Design}  
As discussed the level design would be rooms bundled into floors, each floor representing a different level of complexity on Blooms Taxonomy. For this design chapter I will briefly discuss what some of the lessons in the rooms on the first floor would look like, what their content will be, their learning outcomes and then go on to discuss how I would achieve this. 
\subsubsection{Caesar cipher} 
%discuss the learning outcomes e.g teach what caesar cipher is, how to open up decoder, teach how to decode. Discuss that it would be MCQ. Mock up how it would look. How i will use api request and web scraping to do. 
One of the first things that would be covered in the application is basic encryption types, such as the Caesar cipher.  

\textbf{Learning Outcomes} 
\newline the learning outcomes for this particular lesson would be:  

\begin{itemize}\itemsep0pt
	\item The user would know and understand what a Caesar cipher is 
	\item The user will be able to use an online decoder to get cipher text into plain text
\end{itemize} 

\textbf{Lesson Contents} 
\newline The user would first in a text terminal be told what a Caesar cipher is and would be explained how encryption and decryption work using it. The user would then be given a random Caesar cipher of a famous computer scientist and told to open a online decoder. Using the decoder and referring to the provided steps the user will be able to get the plain text and when entered in the terminal would unlock the door.

\textbf{How this would be done} 
\newline Dynamic challenge generation is an important part of this application and would ensure that the students all have a unique experience meaning users can't copy off of each other's answers and will have a different experience if playing again. This is discussed furthermore later in the design chapter. For this lesson I will make test scenarios using random selection and API requested methods, these will also be discussed further later in the design chapter. 

\textbf{How it would look} 
%insert figure here

 


 
\subsubsection{Hash crack}   
%discuss the learning outcomes e.g teach what md5 hash is that its a one way encryption so only way to crack is against dictionaries provide dictionarie then teach them to find which famous computer scientist it is.  Mock up how it would look. How i will use web scraping to do.  

Another lesson that would be covered on the first floor of the application is hashing. 

\textbf{Learning Outcomes} 
\newline the learning outcomes for this particular lesson would be:  

\begin{itemize}\itemsep0pt
	\item The user would know and understand what a hash is 
	\item The user will be able to use the command line 
	\item The user will, with instructions, be able to crack a hash using a dictionary and john the ripper. 
\end{itemize} 

\textbf{Lesson Contents} 
\newline The user would first in a text terminal be told what a hash is, about one way encryption and the use of it. Then the user will be taught briefly about the command line, basic commands and about john the ripper. the user will then get given a famous computer scientist hashed and be guided with a set of instructions on how to run the hash against a dictionary of computer scientist to get a match using john the ripper. Once the user find a match they will enter it and the door will unlock.

\textbf{How this would be done} 
\newline For this lesson I will make test scenarios using the web scraping method, this will also be discussed further later in the design chapter. The lesson would then have a dictionary full of the hash value of different computer scientist provided to the user.  

\textbf{How it would look} 
%insert figure here
 

\subsubsection{Who is this ?}
%discuss the learning outcomes e.g teach young audience maybe who some famous computer scientists are and why they are famous. Discuss how i will make it get progressively harder until at the end they win reward token. mock up how will look. Discuss how i will use face api, bing api and webscraping to do this.  

Also a possible lesson on the first floor would be a simple "who is this famous computer scientist" stage.  

\textbf{Learning Outcomes} 
\newline the learning outcomes for this particular lesson would be:  

\begin{itemize}\itemsep0pt
	\item The user will be able to recognise famous computer scientists.
\end{itemize} 

\textbf{Lesson Contents} 
\newline The user will be presented with a question "which one of these people is *insert person here*" , four pictures will then be displayed on the screen, one being the right person and one the three others being other computer scientist. When the user gets this right they will proceed to stage 2 and be asked again, this time it will be slightly harder, a less famous computer scientist. Finally if they get that correct they will proceed to the third stage with a lesser known computer scientist. By answering the third question correct the lesson would be complete and the door would unlock. An incorrect answer will result in the user having to go back to the first stage again. This technique of try and try again has been shown to be effective for imparting knowledge (find reference I found for this and insert). 

\textbf{How this would be done}  
\newline For this lesson I will make test scenarios using the web scraping method with the use of two API's, which I will go into more detail about later in this chapter.  

\textbf{How it would look} 
%insert figure here
 

%how this would look in the application etc mock up design 


\subsection{Technical Design} 
\subsubsection{Front end: User Interface}  
The user interface will take into account the literature and represent the application in the most effective way possible. Some drafted screens are shown below.  (Figure \ref{Mainscreen})

\begin{figure}[h]
    \centering
    \includegraphics[width=1.0\textwidth]{Figs/Ui_main_screen.PNG} 
    \caption{Main Screen UI mock up} 
    \label{Mainscreen}
\end{figure}   

The main screen has quite a lot going on. In a typical room, the player will have a choice to navigate both to the next room and the previous room to their left and right. If they go up they will reach the challenge terminal which when interacted with brings up the challenge. If they enter the correct answer the reward room will open up and they can collect their challenge token. If they get it incorrect the hint room below opens up. How many Challenge tokens they have is displayed and also the score they have accumulated so far. The user can quit or alter their settings with the drop down menu in the top left hand corner of the screen. (Figure \ref{Mainmenu}) (Figure \ref{fig:test1}) (Figure \ref{fig:test2})

\begin{figure}[h]
    \centering
    \includegraphics[width=1.0\textwidth]{Figs/Ui_main_menu.PNG} 
    \caption{Main Menu UI mock up} 
    \label{Mainmenu}
\end{figure}   

\begin{figure}
\centering
\begin{minipage}{.5\textwidth}
  \centering
  \includegraphics[width=1\linewidth]{Figs/Ui_hint_room.PNG}
  \captionof{figure}{Hint room}
  \label{fig:test1}
\end{minipage}%
\begin{minipage}{.5\textwidth}
  \centering
  \includegraphics[width=1\linewidth]{Figs/Ui_reward_room.PNG}
  \captionof{figure}{Reward room}
  \label{fig:test2}
\end{minipage}
\end{figure}   

When the user enters the hint room they are presented with a question and a choice of three answers. They will get the hint regardless but is meant to build their knowledge and whats been covered. The reward room is simple and just contains the challenge token. (Figure \ref{fig:test3}) (Figure \ref{fig:test4})

\begin{figure}
\centering
\begin{minipage}{.5\textwidth}
  \centering
  \includegraphics[width=1\linewidth]{Figs/Ui_correct_response.PNG}
  \captionof{figure}{correct response screen}
  \label{fig:test3}
\end{minipage}%
\begin{minipage}{.5\textwidth}
  \centering
  \includegraphics[width=1\linewidth]{Figs/Ui_incorrect_response.PNG}
  \captionof{figure}{incorrect response screen}
  \label{fig:test4}
\end{minipage}
\end{figure}  

The feedback received at this level is basic but dependent on what the answer is different doors unlock.

%insert annotated mockflow screens%
{Register/Home screen} 
{Room screen} 
{Reward screen} 
{Hint screen} 

\subsubsection{Middle end: Dynamic Challenge generator}  
One fundamental part of this application will be the challenge generator. Dynamic challenge generation is an important element of this application, it will ensure that if the user comes back and does it again they will have a different experience and also that users will be having a different experience than the users around them. I will look at three possible methods of dynamic challenge generation. \emph{Random challenge selection, API request} and \emph{Web scraping}.  

\emph{Random Challenge selection} 

This will involve setting up a list of possible questions with corresponding answers, then selecting one of those questions at random. I will ask a multiple choice question populated with the correct answer and the answers to the other questions. The script would them check for the user input and check if the answer matches the correct answer giving simple feedback accordingly. This will be the simplest method and the easiest to implement (Figure \ref{RandomSelection}).  

\begin{figure}[h]
    \centering
    \includegraphics[width=0.5\textwidth]{Figs/random_selection (1).png}
    \caption{random selection flow chart} 
    \label{RandomSelection}
\end{figure}   


\emph{API request} 
I will set up an API using a python script so I can make a request to it with another script. The script will select a random word from a list, the world will correspond with a question that the API will send back when it receives this word. The user will then send back the answer which will be sent to the API and checked (Figure \ref{APIRequest}).   

\begin{figure}[h]
    \centering
    \includegraphics[width=0.5\textwidth]{Figs/API_request.png} 
    \caption{API request flow chart} 
    \label{APIRequest}
\end{figure}   


\emph{Web Scraping} 
I will develop two web scraping applications to test the effectiveness of it as a method of dynamic challenge generation, one simpler one and one more complex. for the more simple one I will scrape the names of influential computer scientists from sites like either 'https://www.computerscie
ncedegreehub.com/30-most-influential-computer-scientists-alive-today/' this or a Wikipedia entry on something similar. Then will read those names into a list. The challenge would be generated by grabbing a random name from that list and running it through an online hash generator, providing all the information you need for the challenge (Figure \ref{WebScraper}).   

\begin{figure}[h]
    \centering
    \includegraphics[width=0.5\textwidth]{Figs/Web_scraper.png} 
    \caption{Web scraper flow chart} 
    \label{WebScraper}
\end{figure}   


The more complex use of web scraping would involve using the Bing API and the Microsoft cognitive engine. The script would ask the Bing API for 100 cryptographers, it would then store images of them in a list, returns one at random and sends it to the Microsoft cognitive engine to see if it recognised it. This script would be used to ask the users if they recognise the famous cryptographers and check their answer against the answer returned from the Microsoft cognitive engine. (Figure \ref{FaceQuiz})   

%talk more about how it would work and then make diagram showing how it works%

\begin{figure}[h]
    \centering
    \includegraphics[width=0.75\textwidth]{Figs/Face_Quiz (1).png}
    \caption{Face Quiz flow chart} 
    \label{FaceQuiz}
\end{figure}     

So the script would select a random search term from a list of possible topics. It would then make a call to the Bing API which will return 100 images of the search term. One of these images will be selected at random and be sent to the Face API, the Face API sends the response back in a JSON format. This will loop until a match is found and the Face API matches a name to the face, when a match has been found the image is displayed and the user will be asked who it is. This process is illustrated in (Figure \ref{APIstructure})  

\begin{figure}[h]
    \centering
    \includegraphics[width=0.75\textwidth]{Figs/API_structure.png} 
    \caption{API structure} 
    \label{APIstructure}
\end{figure}   

\emph{Docker} 
%talk about use of docker and include figure showing how the api is going to be used to send data to database  
 
Docker is a way of systematically automate the faster deployment of applications using portable containers. Docker containers are created using base images, images are created manually are with Dockerfiles. Dockerfiles contain instructions to be automatically performed. They are used to organise deployment artefacts and simply the deployment process \cite{bernstein2014containers}. I will be using this service to populate the challenge database. I will do this by making a script (similar to the Face Quiz script) that instead of opening an image and asking the user who it is being displayed, when a match is found inserts into the challenge database the "question" ("who is this ?"), the image url and the answer (the name of the person), this script is shown in (Figure \ref{DockerFace}) and the structure of the script is shown in (Figure \ref{Dockerstructure}). I set up a Dockerfile to run this script and insert data into the database. Then using docker I could spin up X amount of containers and the database would be populated with X entries. An illustrated example of spinning up 5 container is shown in (Figure \ref{Dockerexample}). By designing it in this manor the database of challenges could be populated in a fast and efficient manor, which would also save processing time when retrieving challenges instead of having to perform the facial matches and image searches at the time of running the script the script could then just retrieve a set of images from the challenge db.
 


\begin{figure}[h]
    \centering
    \includegraphics[width=0.5\textwidth]{Figs/Docker_face_pop.png} 
    \caption{Docker face pop flow chart} 
    \label{DockerFace}
\end{figure}     

\begin{figure}[h]
    \centering
    \includegraphics[width=0.75\textwidth]{Figs/Dbinsert_structure.png} 
    \caption{Database population structure} 
    \label{Dockerstructure}
\end{figure} 

\begin{figure}[h]
    \centering
    \includegraphics[width=1.0\textwidth]{Figs/Docker.png} 
    \caption{Docker Container example} 
    \label{Dockerexample}
\end{figure}

\subsubsection{Back end: Databases} 
\textbf{SQL vs NoSQL}   
First off when designing the back end of an application what Database would be most efficient to use for the applications specific needs and potential uses. 
%reference and and back up use of NOSQL and mongo DB (also discuss that managing the database part of the application when it comes to scaling because a lot of the load comes from complex queries making requests take longer, so techniques such as cacheing complex query results and effective organising of db tables with keys and index can be effective in reducing load. %
\textbf{User Database}   
Discuss the user database (Figure \ref{UserDB})

\begin{figure}[h]
    \centering
    \includegraphics[width=0.75\textwidth]{Figs/User_Db.PNG} 
    \caption{User Database} 
    \label{UserDB}
\end{figure}   

\textbf{Challenges Database}   
Discuss The challenge Database (Figure \ref{ChallengeDB})

\begin{figure}[h]
    \centering
    \includegraphics[width=0.25\textwidth]{Figs/Challenge_Db.PNG} 
    \caption{Challenge Database} 
    \label{ChallengeDB}
\end{figure} 

\textbf{Results Database}   
Discuss the results database (Figure \ref{ResultsDB})

\begin{figure}[h]
    \centering
    \includegraphics[width=0.5\textwidth]{Figs/Result_Db.PNG} 
    \caption{Result Database} 
    \label{ResultsDB}
\end{figure} 


%bit of research on load balancing e.g different algorithms such as random, round robin, and load based.  




\subsection{Design Methodology} 
%discuss what i am going to do pick 100 computer scientists of 3 levels of fame etc etc   
With the application theoretically designed and elements of the application discussed I will now look at how I am going to test and evaluate these prototyped elements after they have been implemented to get useful data out of this project. To do this I am going to test 3 different aspects of my implemented elements. 
\subsubsection{How many times does the same question come up in the same run ?} This will be testing the effectiveness of the implemented dynamic challenge generation methods and how effective each method is for randomly generating a random question. This will also answer the question of what method is superior for this intended purpose, or has the most potential for this use.  

To do this I will for each script I have made for each of the dynamic challenge generation methods, make a separate version for testing that runs through the script 100 times collecting the questions and answer pair each time then after it has ran 100 times it will look at the list of questions and answers and find how many times the same pair occurred in the run. By doing this it gives a numerical value for comparison across all of the methods and helps gauge how effective that particular method is for this chosen purpose and therefore its potential. 
\subsubsection{How many out of 100 images are faces ?}  
This test will be testing the effectiveness of the Bing API that I have chosen to use in the implementation of one of the dynamic challenge methods and for use in populating a database with challenges. By testing this it will tell us how useful this technology is and if it can be utilised for this purpose effectively. 

To test this I am gong to make a test case of the script that uses the Bing API to collect faces for the challenge generation. In this proposed script a random topic such as 'famous computer scientists' or 'famous cryptographers' and then that is used as a search term for the Bing API which returns 100 images of that. I want to test how many of these 100 images are actually contain faces, or more accurately how many actually contain people. I will test this by using search terms with a varying likely hood to return faces, for example,  'Famous movie stars' is very likely to return a set of actors as they by nature are well known but a search term of 'famous cryptographers' on the other hand aren't as well known by nature. Doing this will not only evaluate how useful the Bing API if for general use but analysing different topics it can also be seen how useful it is for the specific  proposed purpose 
\subsubsection{Out of 100 computer scientists, how many does the face API recognise ?} 
This will test how effective the Microsoft cognitive engine Face API, used in my proposed web scraping face quiz script, is at recognising faces.  I believe this could be a very interesting test to carry out and the data gathered from doing this would not only be useful in learning how effective it would be to use this technology for the proposed purpose but for analysing exactly how good this technology is and help determine how it is working. 

To achieve this I am going to make a separate script all together for this test. I am going to gather a list of 100 computer scientists separated into three levels 'world famous' e.g Bill Gates, Steve Jobs, 'industry famous' and 'well known'. I will then gather multiple images of each person on the list and do 10 runs of each of the images.Outputted will be how many of the 3 images were recognised or a 'recognition score' and how many times out of the 10 runs that API recognised the person or a 'consistency score', this will be outputted in a graphical format such as a csv file. I will also feature in the tests some 'dud' images of for example people who look like Steve jobs but actually are not to test how accurate the service is being. As mentioned before it is my hope that these tests will determine how useful the Face API is and help pinpoint its behaviour.
\section{Implementation}
After the research had been done and I had designed in theory what my application would look like and how it would work I could start to look at the implementation. I decided for my implementation to prototype certain parts of my theoretical design. I chose to prototype the dynamic challenge generation part of the project as I felt that it was an important part of the learning experience to make sure that a different challenge could be given to a user each time they used the application, or if it was used in a class to ensure that all the users had different experience's using the application and couldn't copy other users answers out of laziness. Dynamic challenge generation was also an interesting area to look at I thought it would be interesting to evaluate the effectiveness of the different approaches in creating a different challenge each time. Another part I decided to prototype was the docker element of the project as docker is an incredibly useful service which would benefit the project greatly if made and prototyping an element such as database population would help illustrate that as well as gauge how efficient it would be.  

For the implementation stage I decided to use Python for the programming language and Eclipse as the IDE. I chose python out of a mix of personal preference as I has used in in the years leading up to this project and had more experience than any of the other programming languages and the practicality of using it for this project as python has extensive support libraries, useful for completing the variety of tasks that I wanted to do, combined with the fact that it is a widely used language and recognised in the industry it seemed like a sensible choice. Eclipse I chose to use as I have has exposure with it in my uni career and thought it better to use something I am experiences with and know than learn the specifics of a new IDE.

I will use the Eclipse IDE to develop python scripts for the different methods of dynamic challenge generation, at least a script for each. I will then use a combination of docker and python to create a service which populates a database with challenges in an efficient matter. The aim of this implementation stage is to develop something that in a later stage will give an understanding into which method of dynamic challenge generation is the most effective and how to best distribute this method.   

%include code snippets here, explain what they do etc. all scripts and docker files

%need to talk about testing strategy, how am I going to test it ? 
\subsection{Starting simple: From selection}  
\textbf{random\textunderscore selection.py}  

This script was designed to be the simplest method of dynamically generating a challenge, by just selecting from a list of question and answer pairs. the code shown in (Figure \ref{Random1}) takes care of that. First off a dictionary is defined of "caesar questions" is defined, with a key and value of the question and answer. A variable of question is then defined as a random choice of that dictionary and is printed. A for loop then grabs all of answers, correct and incorrect, and prints them before reading in the users response to an answer variable. 

\begin{figure}[h]
    \centering
    \includegraphics[width=1.0\textwidth]{Figs/random1.PNG} 
    \caption{Question selection} 
    \label{Random1}
\end{figure}   

A function is then defined that loops through all of the items in "caesar questions" and compares the answer given by the user to the answer pair to all of the questions. If a match is found then the value "correctAnswers" is set to the corresponding question to the answer given, otherwise it remains empty. Finally the "answerChecked" value is set to what has been returned by that function, if this is equal to the question given at the start "correct" is printed if not "incorrect" is printed. The code for checking the answer is shown in (Figure \ref{Random2}).

\begin{figure}[h]
    \centering
    \includegraphics[width=1.0\textwidth]{Figs/random2.PNG} 
    \caption{Answer Check} 
    \label{Random2}
\end{figure} 


\subsection{Using API request}   

I then began to implement the API method of dynamic challenge generation. In practice it would be more useful to do this with an actual website or service designed for this use but for test purposes I just made a script to represent the API being called to for the answers.  

\textbf{API.py}    

First I implemented the API that was to be called for a question. I set up an API that when ran would run on local host using python Flask. I then set a route for the request to be sent and read in a string that will represent the question to be sent back. depending on the value of the question string the API selects a question answer pair and returns it. The code for this is show in (Figure \ref{API}).


\begin{figure}[h]
    \centering
    \includegraphics[width=1.0\textwidth]{Figs/API.PNG} 
    \caption{API} 
    \label{API}
\end{figure}

\textbf{API\textunderscore request.py} 
Next up I implemented a script that would make a call to this API to get a question shown in (Figure \ref{API request}) . First off a set of tags to be sent are defined in "questions" and "question" is set to a random choice of them. a GET request is then sent to the API and the response is read in, each value contained within the json response is also read into "values". the "user question" and "user answer" variables are set to the question contained in the response and the answer contained in the response respectively, the question is then is then asked to the user and their answer compared to the answer. 

\begin{figure}[h]
    \centering
    \includegraphics[width=1.0\textwidth]{Figs/API request.PNG} 
    \caption{API Request} 
    \label{API request}
\end{figure}

\subsection{Web scraping} 
\subsubsection{Basic web scraping}   
For my more basic attempt at a web scarper I decided to found a website listing off "30 influential computer scientists" and used it to pick them off and use them to generate questions. a hard coded hash method is shown and then a method of opening an online hash generator and putting the text through that as well. 

\textbf{Web\textunderscore scraper\textunderscore final.py}  
Foe this web scraping application I used the BeautifulSoup library for the opening of the website and getting names and the selenium library for opening open a browser and putting the text through an online hash generator. First off (Figure \ref{Webscrape1}) I set the source to the website I wanted to use and defined an empty list "people". Then I take the content returned from the web page and sift through it to find what I want. I inspected the web page and found where the headers were that I wanted to pick out and split the content according to that. After I almost had the text down to the strings I wanted I filtered it further and appended the filtered string to a list of people before "popping" some excess items that came after the desired list of people. A random "person" is selected from the list of "people" found from the website and after this a SHA256 hash is generated of the person from the list and displayed. 

\begin{figure}[h]
    \centering
    \includegraphics[width=1.0\textwidth]{Figs/web_scrape1.PNG} 
    \caption{Web scraping the names} 
    \label{Webscrape1}
\end{figure}  

Then with code shown in (Figure \ref{Webscrape2}) I opened up a web page that calculates md5 hashes as a value is typed in. By inspecting the web page I identified the elements I would need such as the text box to enter the value to be hashed, which I set as "hashField" I also identified the area which the calculated hash is displayed and set that as "hashResult". I sent the "person" that was chosen to the text field "hashField" and once the value was entered read the value of what was in the returned hash field to "hashResult". I then printed this value along with the original person.

\begin{figure}[h]
    \centering
    \includegraphics[width=1.0\textwidth]{Figs/web_scrape2.PNG} 
    \caption{Getting hash through browser} 
    \label{Webscrape2}
\end{figure} 



\subsubsection{Web scraping with facial recognition}   

For this approach I took advantage of Microsoft Azure and used two API services. The Bing search API and the Computer Vision API which uses the Microsoft cognitive engine. I set up both of them on a Microsoft Azure account I created shown in figure (Figure \ref{Azure}) and used them to generate the keys and the links I needed to make my API calls in the script. This script will use the Bing search API to find 100 images of a search term then use the Vision API to recognize a face contained in an image from the search, if it matches a face to the name then it displays to user and asks who they are. 

\begin{figure}[h]
    \centering
    \includegraphics[width=1.0\textwidth]{Figs/Azure.PNG} 
    \caption{Microsoft Azure Cognitive Services} 
    \label{Azure}
\end{figure}

\textbf{Face\textunderscore quiz\textunderscore final.py} 

At the start of the script I declare variables such as a list of random cyber security topics to be chosen from such as "famous computer scientists" or "famous cryptographers" , a search term to be used for the Bing API which is a random choice of those topics and other variables used for the APIs such as subscription keys and headers and params etc. Then I define the findImages function shown in (Figure \ref{findimages}). In this function I set up the API client using the credentials set, then call the API with the search term decided and a count of 100 set as well as a filter of face applied. I then choose a random image and return the image url. 

\begin{figure}[h]
    \centering
    \includegraphics[width=1.0\textwidth]{Figs/findimages.PNG} 
    \caption{findImages function} 
    \label{findimages}
\end{figure}  

Next up I defined the findMatch function shown in figures (Figure \ref{findmatch1}) and (Figure \ref{findmatch2}). First I declare the search term to be used within the function and call the findImages function with that search term, which as discussed, returns an image url that is concatenated in the "body" variable used to send a request to the vision API. The connection to the vision API is set up and a request is sent using that image url as the body. What gets returned is a json response which is broken down into data and is used in the next part to determine whether a match has been found or not. This is done handling certain errors which would mean a match has not been found. I did multiple runs and caught the different errors that were happening if a match wasn't found. For example if the categories or celebrities bracket was empty it meant that there was no match found for the face and when a name value was attempted to be read in and error would happen, with these errors that mean no match had been found the string "no match" is returned at their occurrence. Other exceptions to note was sometimes a match is found but when the image url was trying be accessed the website denied permission and sometimes it would return an image of multiple people which obviously isn't helpful when trying to ask a user who they are as it could be confusing as to which person the question would be referring to, this was solved by accessing the classification parameter and only accepting pictures that were wither of a person or a portrait of a person (dropping images classified as group images). These exceptions were handled and also returned "no match" if they occurred. If none of these exceptions occur then it means a match has been found and a name has been able to be matched to the face so the image url is returned.

\begin{figure}
\centering
\begin{minipage}{.5\textwidth}
  \centering
  \includegraphics[width=1\linewidth]{Figs/findmatch1.PNG}
  \captionof{figure}{findMatch function pt 1}
  \label{findmatch1}
\end{minipage}%
\begin{minipage}{.5\textwidth}
  \centering
  \includegraphics[width=1\linewidth]{Figs/findmatch2.PNG}
  \captionof{figure}{findMatch function pt 2}
  \label{findmatch2}
\end{minipage}
\end{figure}   

After this the getAnswer function is defined, as shown in (Figure \ref{getAnswer}). Firstly I set up the vision API to be used for the question, then sends an image to it, the vision API responds and the name is taken from the response and stored in a variable "name", the certainty score is also taken and rounded up and stored in a variable. Then the image is opened and the user is asked who it is, if they're answer matches the name determined from the API "correct" is returned if not "incorrect" is returned. 

\begin{figure}[h]
    \centering
    \includegraphics[width=1.0\textwidth]{Figs/getanswer.PNG} 
    \caption{getAnswer function} 
    \label{getAnswer}
\end{figure}  

A variable of correct answers is set to 0. The finally inside a for loop that iterates 5 times, is a while loop that calls the findMatch function setting the variable "text" to the value it returns. The while loop breaks when the value returned is not equal to "no match", so essentially, if the image has not triggered any of the exception and a name has been mapped to the face, when this happens "text" is set to the image url and the while loop is broken. The "body" variable is then defined with that image url and the getAnswer function is called asking the user who is in the image and determining if they are correct. If the function returns correct answer then the correct answer variable is incremented by 1. After this process has been iterated through 5 times a score out of 5 is given and the script terminates. The code talked about in this final section is shown in (Figure \ref{askquestion}).

\begin{figure}[h]
    \centering
    \includegraphics[width=1.0\textwidth]{Figs/askquestion.PNG} 
    \caption{Asks user question and iterates} 
    \label{askquestion}
\end{figure} 




\subsection{Challenge distribution with Docker} 
\subsubsection{Python}   
This python script was adapted from the face quiz script but with the use of the pymongo library used to insert challenge data into a MongoDb database. 

\textbf{"Docker\textunderscore face\textunderscore pop\textunderscore final.py"}   

The Docker face population script is very similar to the Face Quiz script discussed in the previous section. The only difference is that it doesn't have the getAnswer function as there is no user interaction and is therefore unnecessary to ask the user the questions and iterate through the process 5 times at the end. The other difference is the use of the pymongo library which is used to when the findMatch function determines there has been a match on the face, takes the "question" that would be asked, the image url for the selected image and the "name" of the person in the image (or the answer) and inserts it into the challenge Database. The lines of code used to establish the connection with the MongoDb and set up the variables (the "db" refered to as the host on line 43 is established in the docker compose file and is discussed later) needed for the insert is shown in (Figure \ref{dbinsert1}). The line that was inserted into the findMatch function, after it was determined to be a match is shown in (Figure \ref{dbinsert2}).

\begin{figure}[h]
    \centering
    \includegraphics[width=1.0\textwidth]{Figs/dbinsert1.PNG} 
    \caption{Establishing Database connection} 
    \label{dbinsert1}
\end{figure}  

\begin{figure}[h]
    \centering
    \includegraphics[width=1.0\textwidth]{Figs/dbinsert2.PNG} 
    \caption{Inserting data into the database} 
    \label{dbinsert2}
\end{figure} 

\subsubsection{Docker}   
These are the files used to create Docker images and spin up the containers with my script running inside them so I can populate the database. 
\textbf{requirements.txt}   
This file is essentially just a list of all the external libraries used in the script that the machine will be running. The Dockerfile uses this file to pip install everything to contained inside. My requirements.txt is shown in (Figure \ref{req}) 

\begin{figure}[h]
    \centering
    \includegraphics[width=1.0\textwidth]{Figs/requirements.PNG} 
    \caption{Requirements.txt contents} 
    \label{req}
\end{figure} 

\textbf{Dockerfile} 
The Dockerfile its the instructions that are needed to build the docker image that will be spinned up in containers.  This included installing the requirements (as discussed in the previous section), copying the python script (docker\textunderscore face\textunderscore pop.py), exposing the port so it can contact the database and finally running. The syntax of this Dockerfile is shown in (Figure \ref{Dockerfile}). 

\begin{figure}[h]
    \centering
    \includegraphics[width=1.0\textwidth]{Figs/Dockerfile.PNG} 
    \caption{Dockerfile contents} 
    \label{Dockerfile}
\end{figure}  

\textbf{docker-compose.yml} 

Docker compose is a tool for defining and running multi container applications which can be used to configure the application. After this has been done all services can be created and started with a single command. I used docker compose to establish the services of "db" (this is the db reffered to in the docker face population script) which is a mongoDB image that I downloaded off of Docker Hub (where you can download from a library of images) and expose the correct ports and set the volume. Then the "client" is defined as using honours-project image (the image created using my requirements.txt and Dockerfile) and link it to the db service so data from the client service can be inserted into the database from the db service. The port is opened and then the environment I will be working with (which database I am going to use etc) is established.


\section{Results} 
%Results for for tests:out of X runs how many times did the same question show up ? to evaluate the three dynamic challenge generation method. test how many faces show up in search out of 100 ? to evaluate bing API effectiveness for different topics such as computer scientists and cryptographers. Out of 100 faces how many correct ? to evaluate facial recognition. Then evaluate further effectiveness of facial recognition by taking 100 famous computer scientists, cryptographers etc of varying fame level, very famous to kind of famous to not very famous and test success rate. feed it some duds, Bill Gates look a like etc.  
\subsection{Test 1: how many times does the same question come up in the same run ?}  
\subsubsection{Making the test cases} 
To perform this test on the different dynamic challenge methods I made altercations to the code of each of the methods so it would return the data I wanted for the purposes of this test. An example of the altercations made is show in (Figure \ref{testcases}) this particular example is taken from the random selection method but the code added to them all was pretty similar, just minor changes depending on how the questions were generated in each of them. These changes consisted of making a list "questions" which each question is added to as the script iterates though the process, "unique\textunderscore question" is another list declared which will be added to every time a question appears and hasn't yet been appended to "questions" (if this is the first time the question appears and is therefore unique). "occurrence \textunderscore count" is added to every time a question is retrieved that is in "questions" and therefore has already been asked. it then iterates through the question asking process 100 times. After this "ocount" is declared which is used to add the total number of occurrences for each unique question, this is used at the end of the script to calculate the average occurrence of a question in 100 runs. Also printed out is how many times each question showed up in that run and how many appearances of the same question there was in that run. Some sample output which I used to illustrate the results is shown in (Figure \ref{exout1}).

\begin{figure}[h]
    \centering
    \includegraphics[width=1.0\textwidth]{Figs/testcases.PNG} 
    \caption{Example of changes made to produce test results} 
    \label{testcases}
\end{figure} 

\begin{figure}[h]
    \centering
    \includegraphics[width=1.0\textwidth]{Figs/exampleoutput1.PNG} 
    \caption{Example output of test cases} 
    \label{exout1}
\end{figure}  

\subsubsection{The Results} 
First I have collected the data and made a graph illustrating the average question occurrence in 100 runs. So for each of the unique questions asked how many times on average was it asked across the 100 runs. This is shown in (Figure \ref{r1}). The Random selection and API request was 25 times, web scraping was 3.44 times and web scraping with web scraping with API was 1.51 times.

\begin{figure}[h]
    \centering
    \includegraphics[width=1.0\textwidth]{Figs/results1.PNG} 
    \caption{Average Question Occurrence} 
    \label{r1}
\end{figure} 

Next I took the number of times that a question that had already been asked was asked for each of the methods and plotted it in the graph shown in (Figure \ref{r2}). Random selection and API request methods had 96 occurrences of the same question, the web scraping method had 71 and the web scraping method using APIs had 34.

\begin{figure}[h]
    \centering
    \includegraphics[width=1.0\textwidth]{Figs/results2.PNG} 
    \caption{Same Question Occurrence} 
    \label{r2}
\end{figure}  

Finally I took both of the results previous tests and plotted them on a graph shown in (Figure \ref{r3}) to show the correlation.

\begin{figure}[h]
    \centering
    \includegraphics[width=1.0\textwidth]{Figs/results3.PNG} 
    \caption{Dynamic Generation Method Results Correlation} 
    \label{r3}
\end{figure} 

\subsection{Test 2: How many faces out of 100 images ?}.   
\subsubsection{Making the test case}
For this test I made a test script which called the Bing API and asked for 100 images with the face tag with four different search terms - "famous people", "famous actors", "famous computer scientists" and "famous cryptographers". The idea being to test the Bing APIs ability to accurately and consistently produce pictures of faces, using different search terms increasing in specificity to gauge how this affects it. I made a test case script for the evaluation of this simply taking a search term and calling the Bing API to return one hundred images with the face tag. I then used the selenium library to iterate through the content url of each of the 100 images returned and open them in a browser. This allowed me to quickly scan through the images looking for ones that did not contain a face. I did this for each search term changing the variable "search\textunderscore term" accordingly each time. The code used for this is shown in (Figure \ref{bingevc})  

\begin{figure}[h]
    \centering
    \includegraphics[width=1.0\textwidth]{Figs/bingevcode.PNG} 
    \caption{Code used for Bing API test case} 
    \label{bingevc}
\end{figure} 

\subsubsection{The Results} 

I collected the results and counted each occurrence of "non face" images in the 100 returned by the Bing API for each search term 3 times to ensure accurate results.Both the "famous people" and "famous movie stars" returned 100 out of 100 images containing faces. "famous computer scientists" returned 93 out of 100 images containing faces and "famous cryptographers" returned 37 out of 100 images containing faces this is illustrated in (Figure \ref{bingev}). 


\begin{figure}[h]
    \centering
    \includegraphics[width=1.0\textwidth]{Figs/bingev.PNG} 
    \caption{Bing API performance} 
    \label{bingev}
\end{figure}



\subsection{Test 3: Out of 100 computer scientists, how many does the face API recognise ?}  

\subsubsection{Making the test case}

To make the test case for the face API evaluation I made alterations to the face quiz application. The first step however to select a list of 100 computer scientists. I gathered 100 names and grouped them by fame level. 10 very famous 15 well known and 75 industry recognised, this list is available in the appendix. After this list was decided I started the altercation by declaring three dictionaries, one for each fame bracket. In this dictionary was three image links for each person in the list, which I sourced myself rather than using the Bing API previously used in the project to ensure the data was consistent, in that they were a similar quality of photo, and accurate, in that the images were of who they were supposed to be. Other changes included adding two functions one similar to the findMatch function used earlier except it returned the certainty score called getScore and a second which just took the digits off of the end of the names of the people (as they were numbered 1-3 so that they were different keys) called mySplit. Then for each of the three dictionaries the following was done: Three variables were declared; “xcorrect” to count how many correct matches, “xscores” to collect the certainty scores of the matched faces and finally “xmatched” which collects the names of all of the people that have been matched. Then a for loop iterates through each of the values in the dictionary trying to find a match to the face, appending to the lists and incriminating the correct counter when a match is made.  After this has been done for all three dictionaries, the results are displayed giving: how many celebrities in that category were matched, the average confidence rating given with matches for that category and the total pictures that were matched.  Some code for this is shown in (Figures \ref{fec1} \ref{fec2} \ref{fec3}).  An example of the output is shown in (Figure \ref{feo}).   

\begin{figure}[h]
    \centering
    \includegraphics[width=1.0\textwidth]{Figs/faceevalcode1.PNG} 
    \caption{Setting up of dictionaries} 
    \label{fec1}
\end{figure} 

\begin{figure}[h]
    \centering
    \includegraphics[width=1.0\textwidth]{Figs/faceevalcode2.PNG} 
    \caption{Appending lists and incriminating count} 
    \label{fec2}
\end{figure} 

\begin{figure}[h]
    \centering
    \includegraphics[width=1.0\textwidth]{Figs/faceevalcode3.PNG} 
    \caption{Formatting data to be output} 
    \label{fec3}
\end{figure} 

\begin{figure}[h]
    \centering
    \includegraphics[width=1.0\textwidth]{Figs/faceevaloutput.PNG} 
    \caption{Example output of Face API test case} 
    \label{feo}
\end{figure} 


\subsubsection{The Results}

For this test fed the Face API 3 different pictures of each of the computer scientists, and did three runs of the script and averaged out the results. The first metric measured was how many people were recognised, the average was 10 out of 10 for the very famous category, 15 out of 15 for the well known category and 58 out of 75 for the industry recognised as illustrated in (Figure \ref{fer1}). This equates to a total of 83 out of 100 computer scientists being recognised. 

\begin{figure}[h]
    \centering
    \includegraphics[width=1.0\textwidth]{Figs/faceevalr1.PNG} 
    \caption{Face API people recognised} 
    \label{fer1}
\end{figure}

For the next part of this test I measured the average picture recognition, meaning the face API might have recognised the computer scientist but how many of the 3 pictures did it recognise ? Essentially how consistent was the Face API across all the 3 fame categories. The averages were 27 out of a possible 30 images recognised for the very famous category, 36 out of a possible 45 images were recognised in the well known category and 120.6 out of a possible 225 images were recognised in the industry recognised category, on average, as illustrated in (Figure \ref{fer2})  

\begin{figure}[h]
    \centering
    \includegraphics[width=1.0\textwidth]{Figs/faceevalr2.PNG} 
    \caption{Face API images recognised} 
    \label{fer2}
\end{figure}

The third and final metric gathered was the average confidence rate of the matches made, the Face API gives a confidence rating along with the match stating how sure it is that it is that person. The average confidence rating for the very famous category was 99.43 percent, the average confidence rating for the well known category was 98.54 percent and the average confidence rating for the industry recognised category was 98.71 percent, this is illustrated in (Figure \ref{fer3}). 

\begin{figure}[h]
    \centering
    \includegraphics[width=1.0\textwidth]{Figs/faceevalr3.PNG} 
    \caption{Face API average confidence ratings} 
    \label{fer3}
\end{figure} 

I then took all of these results and mapped them on a graph to show a correlation, this is shown in (Figure \ref{fer4}) 

\begin{figure}[h]
    \centering
    \includegraphics[width=1.0\textwidth]{Figs/faceevalr4.PNG} 
    \caption{Face API performance} 
    \label{fer4}
\end{figure}

\section{Evaluation}
Evaluate my literature and implementation as a whole. 

\section{conclusion} 
\subsection{Aims and Objectives} 
\subsection{Self Evaluation} 
\subsection{Future Work}

\newpage
\bibliographystyle{IEEEtran}
\bibliography{Diss_papers}
%example of References. See https://en.wikibooks.org/wiki/LaTeX/Bibliography_Management
%might be good to use a separate document for these so your main work is not one really long text file. 

%you can crate this on a extra tex document just like the title or any other part of the document.
\newpage
\begin{appendices}
\section{Project Overview}
%insert IPO

\begin{subappendices}
\subsection{Example sub appendices}
...
\end{subappendices}

\section{Second Formal Review Output}
Insert a copy of the project review form you were given at the end of the review by the second marker

\section{Diary Sheets (or other project management evidence)}
Insert diary sheets here together with any project management plan you have

\section{Appendix 4 and following}
insert content here and for each of the other appendices, the title may be just on a page by itself, the pages of the appendices are not numbered, unless an included document such as a user manual or design document is itself pager numbered.
\end{appendices}

\end{document}
